// maze example (POMDP)
// slightly extends that presented in
// Littman, Cassandra and Kaelbling
// Learning policies for partially observable environments: Scaling up  
// Technical Report CS, Brown University
// gxn 29/01/16

// state space (value of variable "s")

//  0  1  2  3  4
//  5     6     7
//  8     9    10
// 11     13   12

// 13 is the target

pomdp

// can observe the walls and target
observable "west" = s=0|s=5|s=6|s=7|s=8|s=9|s=10|s=11|s=12|s=13; // wall to the west
observable "east" = s=4|s=5|s=6|s=7|s=8|s=9|s=10|s=10|s=11|s=12|s=13; // wall to the east
observable "north" = s=0|s=1|s=2|s=3|s=4; // wall to the north
observable "south" = s=1|s=3|s=11|s=12|s=13; // wall to the south
observable "target" = s=13; //target

module maze

	s : [-1..13];
	
	// initialisation
	[] s=-1 -> 1/13 : (s'=0)
			 + 1/13 : (s'=1)
			 + 1/13 : (s'=2)
			 + 1/13 : (s'=3)
			 + 1/13 : (s'=4)
			 + 1/13 : (s'=5)
			 + 1/13 : (s'=6)
			 + 1/13 : (s'=7)
			 + 1/13 : (s'=8)
			 + 1/13 : (s'=9)
			 + 1/13 : (s'=10)
			 + 1/13 : (s'=11)
			 + 1/13 : (s'=12);
	
	// moving around the maze
	
	[east] s=0 -> (s'=1);
	[south] s=0 -> (s'=5);

	[east] s=1 -> (s'=2);
	[west] s=1 -> (s'=0);

	[east] s=2 -> (s'=3);
	[west] s=2 -> (s'=1);
	[south] s=2 -> (s'=6);

	[east] s=3 -> (s'=4);
	[west] s=3 -> (s'=2);

	[west] s=4 -> (s'=3);
	[south] s=4 -> (s'=7);

	[north] s=5 -> (s'=0);
	[south] s=5 -> (s'=8);

	[north] s=6 -> (s'=2);
	[south] s=6 -> (s'=9);

	[north] s=7 -> (s'=4);
	[south] s=7 -> (s'=10);

	[north] s=8 -> (s'=5);
	[south] s=8 -> (s'=11);

	[north] s=9 -> (s'=6);
	[south] s=9 -> (s'=13);

	[north] s=10 -> (s'=7);
	[south] s=10 -> (s'=12);

	[north] s=11 -> (s'=8);

	[north] s=12 -> (s'=10);

	// loop when we reach the target
	[done] s=13 -> true;

endmodule

// reward structure (number of steps to reach the target)
rewards

	[east] true : 1;
	[west] true : 1;
	[north] true : 1;
	[south] true : 1;

endrewards
